%!TEX root =  flsa.tex
\section{Instance Dependent Bounds}\label{sec:mainresults}
Existence of instance dependent convergence rate and constant step-size are necessary before we can proceed to questions about existence of \emph{universal} step-size and \emph{uniform} rate. We show that any  CALSA scheme satisfying \Cref{assmp:lsa} achieves an asymptotic rate of $O(\frac{1}{t})$ with a instance dependent step-size (see \Cref{th:rate}). 
As a first step, we need show that there exists an instance dependent step-size such that the iterates are stable. To this end, it is useful to look the error (un-averaged iterates) $e_t\eqdef\theta_t-\ts$ given as below:
%\begin{align}\label{eq:errrec}
%e_t&=\underbrace{\Pi_{s=1}^t (I-\alpha A_s) e_0}_{\text{bias}}+\alpha\underbrace{\sum_{j=1}^t\Pi_{s=j}^t (I-\alpha A_s)\zeta_t}_{\text{variance}}
%\end{align}
\begin{align}\label{eq:errrec}
e_t&={\Pi_{s=1}^t (I-\alpha A_s) e_0}+\alpha{\sum_{j=1}^t\Pi_{s=j}^t (I-\alpha A_s)\zeta_t}
\end{align}
It is clear that for the iterates to be stable a necessary condition is that the products of random matrices  of the form $\Pi_{s=t'}^{t}(I-\alpha A_s),\,t'=1,\ldots,t-1$ appearing in \eqref{eq:errrec} should not blow up. In \Cref{def:spect} and \Cref{lem:hur} we show that (under \Cref{assmp:lsa}), for any given problem instance $P$, there exists a range of step-sizes for which this necessary condition can be satisfied.
\begin{definition}\label{eq:}
Let $A_t$ be as in \Cref{assmp:lsa}, and $U\in\C^{\dcd}$ be any invertible matrix. Now define $\Lambda_t\eqdef U^{-1}A_t U$, $\Lambda\eqdef \EE{U^{-1}A U}$,
\begin{align*}
&\rhos{P_U}\eqdef{\inf}_{\underset{\norm{x}=1}{x\in \C^d}}\EE{\ip{x,\left((\Lambda+\Lambda^*)-\alpha \Lambda_t^*\Lambda_t\right)x}}\,\\
& \rhod{P_U}\eqdef{\inf}_{\underset{\norm{x}=1}{x\in \C^d}}{\ip{x,\left((\Lambda+\Lambda^*)-\alpha \Lambda^* \Lambda\right)x}},
\end{align*}
\end{definition}
where in $\rhod{P_U}$ and $\rhos{P_U}$, the subscripts $d$ and $s$ stands for \emph{deterministic} and \emph{stochastic} respectively. Also, note that $\rhos{P_U}<\rhod{P_U}$ holds from Jensen's inequality.
\begin{comment}
In order to produce our instance dependent bounds, we first need to bound the growth rate of product of random matrices. Our approach here is to use the following expression: \begin{align}\label{eq:spectineq}
\EE{\norm{\Pi_{s=1}^t(I-\alpha A_s)}}\leq \EE{\norm{(I-\alpha A_1)}}^t,
\end{align}
where the inequality follows from fact that $\norm{AB}\leq \norm{A}\norm{B}$ for matrices $A, B \in \R^{\dcd}$, and the $\iid$ assumption on $A_t\,$s. Thus, based on the inequality in \eqref{eq:spectineq}, a sufficient condition to ensure stability is to ensure that the per-iteration spectral norm is less than unity $\EE{\norm{I-\alpha A_1}}<1$, then it would be sufficient to ensure stability. However, this sufficient condition could hurt us, in that, there exist stable products of \emph{Hurwitz} matrices whose per-iteration spectral norm is greater than unity. However, it is easy to remedy this situation (see \Cref{lm:hur)} by applying an appropriate change of basis first and then checking for the sufficient condition. Thus, we consider the following recursion:
\begin{align*}
\gamma_t=(I-\alpha \Lambda_t) \gamma_t +\alpha U^{-1}b_t,
\end{align*}
where $U\in C^{\dcd}$ is a change of basis matrix, $\gamma_t=U^{-1}\theta_t$, and $\Lambda_t=U^{-1}A_t U$. Now, defining $e^U_t\eqdef \gamma_t-\gamma_*$, where $\gamma_*=U^{-1}\theta_*$, we have:
\begin{align}\label{eq:trnserrrec}
e^U_t=(I-\alpha \Lambda_t)e^U_t+\alpha U^{-1}\zeta_t,
\end{align}
where $\zeta_t$ is as in \eqref{eq:errrec}. Note that since $\theta_t=U\gamma_t$, we can ensure the stability of \eqref{eq:errrec}, by ensuring stability of \eqref{eq:trnserrrec} whose growth is related to constants defined as follows:
\end{comment}

\begin{lemma}\label{lm:hur}
Under \Cref{assmp:lsa} there exists an $\alpha_{P_U}>0$ and an invertible $U\in \gln$ such that $\rhod{P_U}>0$ and $\rhos{P_U}>0$ holds for all $\alpha \in (0,\alpha_{P_U})$. 
\end{lemma}
In \Cref{lem:hur}, $U$ is a change of basis matrix, and by defining a transformed iterate $\gamma_t\eqdef U^{-1}\theta_t$, we have $\gamma_t=(I-\alpha \Lambda_t) \gamma_t +\alpha U^{-1}b_t$. Then the stability of $\gamma_t$ implies stability of $\theta_t$; the former can be ensured if products $\Pi_{s=t'}^t (I-\alpha \Lambda_s),\,t'=1,\ldots,t-1$ don't blow up. Note that $\EE{\norm{I-\alpha \Lambda_t}^2}=1-\alpha \rhos{P_U}$, and thus \Cref{lem:hur} implies that $\EE{\norm{I-\alpha \Lambda_t}^2}<1,\,\forall \alpha\in (0,\alpha_{P_U})$. This similarity transformation is necessary to transform any \emph{Hurwitz} matrix to an appropriately scaled Jordon canonical form (see \Cref{appendix} for more details). In what follows, we continue with this linear transformation idea to produce the bounds on the MSE for CALSA on a problem instance $P$, by first bounding $\gh_t-\gamma_*$ and then recovering $\thh_t-\ts=U(\gh_t-\gamma_*)$.
\begin{comment}
\begin{proof}
\begin{align*}
\rhos{P}&=\inf_{x:\norm{x}=1}x^* (A_P+A_P^*)x -\alpha x^*\EE{A_t^* A_t} x\\
&=\inf_{x:\norm{x}=1}x^* (A_P+A_P^*)x -\alpha x^* A^*_P A_P -\alpha x^* \EE{M_t^* M_t} x\\
&\geq \lambda_{\min}(A^*_P+A_P)-\alpha \norm{A_P}^2-\sigma^2_P
\end{align*}
The proof is complete by choosing $\alpha_P<\frac{\lambda_{\min}(A^*_P+A_P)}{\norm{A_P}^2+\sigma^2_P}$
\end{proof}
\end{comment}
\begin{comment}
\begin{theorem}\label{th:pdrate}
Let $P^A$ in \Cref{assmp:lsa}-~\ref{dist} be a positive definite distribution over $\C^{\dcd}$. Then
\begin{align*}
\EE{\norm{\thh_t-\ts}^2}
\leq
\left(1+\frac2{\alpha\rhod{P}}\right)\frac{1}{\alpha \rhos{P}}
\left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\theta_0-\ts}}{t+1} \right)\,.
%\left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2{\sigma}_1^2+ {\sigma}_2^2 (\alpha^2+\alpha \norm{\theta_0-\ts})}{t+1} \right)\,.
\end{align*}
\end{theorem}

\begin{theorem}\label{thm:simtran}[Change of Basis]
Let $P$ in \Cref{assmp:lsa}-~\ref{dist} be AS. Define $\gamma_t\eqdef U^{-1}\theta_t,\,,\gamma^*\eqdef U^{-1}\ts$, and suppose if $P_U$ is positive definite, then
\begin{align*}
\EE{\norm{\gh_t-\gamma^*}^2}
\leq
\left(1+\frac2{\alpha\rhod{P_U}}\right)\frac{\norm{U^{-1}}^2}{\alpha \rhos{P_U}}
\left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\theta_0-\ts}}{t+1} \right)\,.
%\left(\frac{\norm{\gamma_0-\gamma^*}^2}{(t+1)^2}+ \frac{\alpha^2(\sigma_P^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_P^2\norm{\ts})\norm{\gamma_0-\gamma^*}}{t+1} \right)\,.
%\left(\frac{\norm{\gamma_0-\gamma^*}^2}{(t+1)^2}+ \frac{\alpha^2{\sigma}_1^2+ {\sigma}_2^2 (\alpha^2+\alpha \norm{\gamma_0-\gamma^*})}{t+1} \right)\,,
\end{align*}
where $\gh_t=\frac{1}{t+1}\sum_{s=0}^t \gamma_s$.
\end{theorem}
\end{comment}

\begin{theorem}\label{th:rate}
Let $P$ be a distribution over $\R^d\times \R^{\dcd}$ satisfying \Cref{assmp:lsa}.
Then, for  $U\in\gln$ and $\alpha_{P_U}>0$ as in \cref{lm:hur},
\todoc{Why not use $\alpha_P$? $U$ is dependent on $P$. If $\alpha_{P_U}$ does not have a specific definition, there is no point to denote this dependence on $U$. It just adds to the clutter.}
for all $\alpha\in (0,\alpha_{P_U})$ and for all $t\ge 0$,
\begin{align*}
\EE{\normsm{\thh_t-\ts}^2}
\leq
\nu\,
\left\{\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{v^2}{t+1} \right\}\,,
\end{align*}
where $\nu = \left(1+\tfrac2{\alpha\rhod{P_U}}\right)\tfrac{\cond{U}^2}{\alpha \rhos{P_U}}$ and
$v^2 = 
\alpha^2(\sigma_A^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_A^2\norm{\ts})\norm{\theta_0-\ts}$.
\end{theorem}
Note that $\nu$ depends on $P_U$ and $\alpha$, while $v^2$ in addition also depends on $\theta_0$. The dependence,  when it is essential, will be shown as a subscript.
\begin{comment}

Consider the LSA with $b_t=0$, and $P^0$ has all the mass concentrated on $A_{P^0}=\begin{bmatrix} \lambda_{\min} &0\\ 0& \lambda_{\max}\end{bmatrix}$, for some $\lambda_{\max}>\lambda_{\min}>0$. Note that in this example $\ts=0$.
By choosing $\alpha<\frac2{\lambda_{\max}}$, in this case it is straightforward to write the expression for $\eh_t$ explicitly as below:
\begin{align*}
\eh_t&=\frac{1}{t+1}\sum_{s=0}^t e_t = \frac{1}{t+1}\sum_{s=0}^t (I-\alpha A_P)^{t-s} e_0\\
&=\frac{1}{t+1}(\alpha A)^{-1}\left(I-(I-\alpha A)^{t+1}\right)e_0
\end{align*}

\begin{align*}
\norm{\eh_t}^2&=\frac{1}{(t+1)^2}\norm{(\alpha A_P)^{-1}\left(I-(I-\alpha A_P)^{t+1}\right)e_0}^2
&\leq\frac{1}{(t+1)^2}\alpha^2\norm{A_P^{-1}}^2\norm{e_0}^2
\end{align*}

\begin{align*}
\norm{\eh_t}^2\geq \eh^2_t(1)=\frac{1}{(t+1)^2}(\alpha \lambda_{\min})^{-2}(1-\alpha \lambda_{\min}^t) \theta^2_0(1)
\end{align*}
Note that in this example, $\rhos{P^0}=\rhod{P^0}=\lambda_{\min} -\alpha \lambda_{\min}^2=\lambda_{\min}(1-\alpha \lambda_{\min})$. Sandwiching the error between the lower and upper bounds we have
\begin{align}
\frac{1}{(t+1)^2}(\alpha \lambda_{\min})^{-2}(1-\alpha \lambda_{\min}^t) \theta^2_0(1)\leq \norm{\eh_t}^2\leq
\frac{1}{(t+1)^2} \left(\alpha^{-1}\rhos{P^0}+\alpha^{-2}\rhos{P^0}^2\right)\norm{\theta_0}^2
\end{align}
\end{comment}

\begin{theorem}[Lower Bound]\label{th:lb}
There exists a distribution $P$ over $\R^d\times \R^{\dcd}$ satisfying \Cref{assmp:lsa}, such that
there exists $\alpha_P>0$ so that $\rhos{P}>0$ and $\rhod{P}>0$ hold for all $\alpha\in (0,\alpha_P)$ and
for any $t\ge 1$, \todoc{$t\ge 1$?} $\EE{\normsm{\thh_t-\ts}^2} 
\geq \frac{1}{\alpha^2 \, \rhod{P}\rhos{P}} \,\left\{ \frac{\beta_t \norm{\theta_0-\ts}^2}{(t+1)^2} 
+ \frac{v^2\sum_{s=1}^t \beta_{t-s}  }{(t+1)^2} \right\}\,,$
\begin{comment}
\begin{align*}
\EE{\normsm{\thh_t-\ts}^2} 
&\geq \frac{1}{\alpha^2 \, \rhod{P}\rhos{P}} \,\left\{ \frac{\beta_t \norm{\theta_0-\ts}^2}{(t+1)^2} 
+ \frac{v^2\sum_{s=1}^t \beta_{t-s}  }{(t+1)^2} \right\}\,,
%&\geq \frac{1}{(t+1)^2}(\alpha)^{-2}(\rhod{P}\rhos{P})^{-1}\Big\{ \big(1-(1-\alpha \rhos{P})^t\big) \norm{\theta_0-\ts}^2 \\
%&+ \sum_{s=1}^t \big(1-(1-\alpha \rhos{P})^{(t-s)}\big) \big(\alpha^2(\sigma_A^2\norm{\ts}^2+\sigma_b^2)+\alpha (\sigma_A^2\norm{\ts})\norm{\theta_0-\ts}\big) \Big\}\,.
\end{align*}
\end{comment}
where $\beta_{t} =  \big(1-(1-\alpha \rhos{P})^t\big)$ and $v^2$ is as in \cref{th:rate}.
\begin{comment}
\begin{align}
\frac{1}{(t+1)^2}(\alpha \lambda_{\min})^{-2}(1-\alpha \lambda_{\min}^t) \theta^2_0(1)\leq \norm{\eh_t}^2\leq
\frac{1}{(t+1)^2} \left(\alpha^{-1}\rhos{P^0}+\alpha^{-2}\rhos{P^0}^2\right)\norm{\theta_0}^2
\end{align}
\end{comment}
\end{theorem}
Note that $\beta_t \to 1$ as $t\to\infty$. Hence, the lower bound essentially matches the upper bound.
%\subsection{Discussion}
In what follows, we discuss in brief the results in \Cref{th:rate,th:lb}. 
\textbf{Bias and Variance}: The MSE at time $t$ is bounded by a sum of two terms. The first \emph{bias} term is given by $\B=\nu \,\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}$, bounding how fast the initial error $\norm{\theta_0-\ts}^2$ is forgotten. The second \emph{variance} term is given by $\V=\nu\, \frac{v^2}{t+1} $ and captures the rate at which noise is rejected. 

\textbf{Behaviour for extreme values of $\alpha$}: As $\alpha\to 0$, the bias term blows up, due to the presence of $\alpha^{-1}$ there. This is unavoidable (see also \cref{th:lb}) and is due to the slow forgetting of initial conditions for small $\alpha$. Small step-sizes are however useful to suppress noise, as seen from that in our bound $\alpha$ is seen to multiply the variances $\sigma^2_A$ and $\sigma^2_b$. In quantitative terms, we can see that the $\alpha^{-2}$ and $\alpha^2$ terms are trading off the two types of errors. For larger values of $\alpha$ with $\alpha_{P_U}$ chosen so that $\rhos{P_U}\ra 0$ as $\alpha\ra \alpha_{P_U}$ the bounds blow up again. 
Use of such similarity transformation have also been considered in analysis of RL algorithms \cite{lihong}. More generally, one can always take $U$ in the result that leads to the smallest bound.

\textbf{Proof Sketch:} We can expand the MSE as $\EE{\norm{\eh_t}^2}=\tfrac{1}{(t+1)^2}\, \ip{ \textstyle\sum_{s=0}^t e_s,\textstyle\sum_{s=0}^t e_s}\,,$
where $\eh_t = \thh_t-\ts$ and $e_t = \theta_t-\ts$, and the inner product is a summation of \emph{diagonal} terms $\EE{\ip{e_s,e_s}}$ and \emph{cross} terms of $\EE{\ip{e_s,e_q}}$, $s\neq q$. Since, we also use a basis transformation via $U$, the growth of the diagonal terms and the cross terms depends on the spectral norm of the random matrices $I-\alpha \Lambda_t$ and that of the deterministic matrix $I-\alpha \Lambda$, respectively. This explains the reason as to why $\rhos{P_U}$ and $\rhod{P_U} $ appear in the bounds.
%\vspace*{0.5em}
%\subsection{Result Discussion}
%\begin{itemize}[leftmargin=*, before = \leavevmode\vspace{-\baselineskip}]


\begin{comment}
\textbf{Role of $U$}: When the given problem $P$ is positive definite, it is straightforward to produce an $\alpha_P>0$ such that $\rhos{P}>0$ and $\rhod{P}>0,\,\forall \alpha\in(0,\alpha_P)$ by considering the definitions of $\rhos{P}$ and $\rhod{P}$.
However, the problem $P$ being Hurwitz does not guarantee positive definiteness. % (see \Cref{ex:pdas}). 
\todoc{I commented out the examples.}
Following  \cite{lihong}, we remedy this 
by applying a similarity transformation using an appropriate $U$ that transforms a Hurwitz problem $P$ to a positive definite problem $P_U$. This can be done because any Hurwitz matrix is similar to a PD matrix. 
Taking $U$ to be this similarity matrix, we transform  \eqref{conststep} by pre-multiplying it by $U^{-1}$ and introducing a new variable $\gamma_t\eqdef U^{-1 }\theta_t$.
\if0
To see how this works, let $\gamma_t\eqdef U^{-1 }\theta_t$. Pre-multiplying 
 \eqref{conststep} by $U^{-1}$ and plugging in $\gamma_t$, we get
\begin{align*}
%U^{-1}\theta_t&=U^{-1}\theta_{t-1}+\alpha(U^{-1} b_t- U^{-1}A_t U U^{-1}\theta_{t-1}),\\
\gamma_t&=\gamma_{t-1}+\alpha(U^{-1} b_t- J_t\gamma_{t-1}),
\end{align*}
where $J_t = U^{-1}A_t U$.
Now, since any Hurwitz matrix is similar to a PD matrix with an appropriate similarity transformation $U$, choosing $U$ this way we get that $\E[J_t]$ is PD, hence $P_U$ is PD.
\fi
%This completes the transformation of a Hurwitz problem $P$ to a positive definite problem $P_U$. 
The condition number $\cond{U}$ appears in our bound because $\theta_t$ is recovered from $\gamma_t$ using
$\theta_t=U\gamma_t$. Of course, when the problem $P$ is positive definite, we can let $U=\I$ in \Cref{th:rate}. 
More generally, one can always take $U$ in the result that leads to the smallest bound.
\end{comment}
%\textbf{Role of $\rhos{P_U}$ and $\rhod{P_U}$}: 
\begin{comment}
The core part of the proof involves analyzing what we call the error dynamics. Define $e_t\eqdef\theta_t-\ts$, then we have
\begin{align}\label{eq:errmain}
\begin{split}
\theta_t&=\theta_{t-1}+\alpha\big(b_t-A_t\theta_{t-1}\big)\\
\theta_t-\ts&=\theta_{t-1}-\ts+\alpha\big(b_t-A_t(\theta_{t-1}-\ts+\ts)\big), \\
e_t&=(I-\alpha A_t)e_{t-1}+\alpha(b_t -b -(A_t-A)\ts)\\
e_t&=(I-\alpha A_t)e_{t-1}+\alpha\zeta_t,
\end{split}
\end{align}
where $\zeta_t\eqdef(b_t -b -(A_t-A)\ts)$. A way to look at \eqref{eq:errmain} it that at each time $t$, the error $e_t$ is multiplied by the random matrix $(I-\alpha A_t)$ and is added by a zero mean noise. Intuitively speaking, the noise part is taken care of by the PR average, and what remains to be investigated is behaviour of the random matrix. In fact, the recursion \eqref{eq:errmain} can be unfurled as below
\begin{align}\label{eq:unfold}
e_t
& = (I-\alpha A_t) (I-\alpha A_{t-1}) e_{t-2}\\ &+ \alpha (I-\alpha A_t) \zeta_{t-1} +\alpha \zeta_t \\
& \quad \vdots\\
& = (I-\alpha A_t) \cdots (I-\alpha A_1) e_0\\ &+ \alpha (I-\alpha A_t) \cdots (I-\alpha A_2) \zeta_1 \\
& + \alpha (I-\alpha A_t) \cdots (I-\alpha A_3) \zeta_2\\
&  \quad \vdots \\
&+ \alpha \zeta_t\,,
\end{align}



\textbf{Constants} {$\alpha$, $\rhos{P}$ and $\rhod{P}$} do not affect the exponents $\frac{1}{t}$ for variance and $\frac{1}{t^2}$ for bias terms. This property is not enjoyed by all step-size schemes, for instance, step-sizes that diminish at $O(\frac{c}{t})$ are known to exhibit $O(\frac{1}{t^{\mu c/2}})$ ($\mu$ is the smallest real part of eigenvalue of $A_P$), and hence the exponent of the rates are not robust to the choice of $c>0$ \cite{bach-moulines,korda-prashanth}.
\todoc{Why is this a great thing? Refer to other works etc.}

\end{comment}


\textbf{The lower bound} of \Cref{th:lb} shows that the upper bound of \cref{th:rate} is tight in a number of ways.
In particular, the coefficients of both the $1/t$ and $1/t^2$ terms inside $\{ \cdot \}$ are essentially matched (here $U=I$).
Further, we also see that the $(\rhos{P_U}\rhod{P_U})^{-1}$ appearing in $\nu = \nu_{P_U,\alpha}$ cannot be removed from the upper bound. 
Note however that there are specific examples, such as SGD for linear least-squares,
where this latter factor can in fact be avoided (for further remarks see \Cref{sec:related}).

To sum up, in \Cref{th:rate} we have established the fact that under \Cref{assmp:lsa} CALSA achieves an \emph{asymptotic} rate of $O(\frac{1}{t})$ for the MSE with an instance dependent step-size range of $(0,\alpha_{P_U})$, and the  constant $\nu$ has instance dependent terms. In the \Cref{sec:land,sec:td}, we investigate conditions under which this instance dependence can be eliminated such that we have \emph{universal} step-size choice and \emph{uniform} rates.


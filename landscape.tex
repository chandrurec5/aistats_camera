\section{Problem Landscape}\label{sec:land}
\begin{comment}
We first consider the question of choosing a \emph{universal} constant step-size which is defined as follows:
\begin{definition} Let $\P$ be a problem class and $P\in\P$ be an instance, then define $\alpha^{stab}_{P}\subset (0,\infty)$ to be such that the random matrix product $\Pi_{s=1}^t (I-\alpha A_s)\ra 0$ (note $(b_t,A_t)\overset{i.i.d}{\sim} P$ ) as $t\ra\infty$ whenever $\alpha\in \alpha^{stab}_P$ and $\alpha^{stab}_{\P}\eqdef\cap_{P\in \P}\alpha_{P}$. Define any $\alpha\in\alpha_{\P}$ to be a \emph{universal} step-size choice for $\P$.
\end{definition}
Note that from \Cref{lem:hur} we have $(0,\alpha_{P_U})\subset \alpha^{stab}_{P}$. 

\textbf{Uniform Rates:} Suppose the set $\alpha^{stab}_{\P}$ is non-empty for a class $\P$, then the question of \emph{uniform} rates boils down to handling the the bias-
variance trade-off in \Cref{th:rate}, say we choose $\alpha\in,\alpha^{stab}_{\P}$: a larger $\alpha$ might mean faster forgetting of bias; a smaller $\alpha$ means lesser noise. In what follows, we investigate the questions of universality and uniformity for simple problem classes define as follows:
\begin{definition}
Define the following \emph{additive noise} problem classes (where $A_t=A_P,\forall t\geq 0$): $(i)$  $\P_{SPD}=\{ P: A_P\in \R^{\dcd}\text{is real symmetric positive definite with~} \Lambda(A_P)<1, b_P=\mathbf{0}, \sigma^2_{b_P}=1\}$, $(ii)$ $\P_{SPDSN}\subset \P_{SPD}$ such that $\EE{b_tb_t^\top}<A_P$ $(iii)$ $\P_{rot}=\{P=(u,v): b_t=\mathbf{0}, A_t=A_P, \forall t\geq 0, A_P=\left[\begin{matrix} u &v \\ -v & u\end{matrix}\right], u^2+v^2\leq B\}$.
\end{definition}
Problem classes $\P_{SPD}$ and $\P_{rot}$ are special cases of \Cref{assmp:lsa}-\eqref{dist}, i.e., they have only \emph{additive} noise through $b_t$. While the additive noise case does not hold for our domains such as RL, we are interested in these classes to understand how problem structure plays a crucial role in universality of step-size and uniformity of rates. In $\P_{SPD}$, the matrices are diagonalizable, all the eigenvalues are real. However, in $\P_{rot}$ the matrices are \emph{Hurwitz} and eigenvalues have non-zero imaginary parts. Further in $\P_{SPDSN}$, the noise \emph{scales} with the underlying $A_P$ matrix. 
\begin{theorem}\label{th:pspd}
Any $\alpha\in(0,1)$ is a universal step-size for $\P_{SPD}$. Let $\B=\norm{e_0}^2$, then, for $t$ such that $\alpha A_P t < I $ $(i)$  $\EE{\norm{\eh_t}^2}\approx \frac{1}{(t+1)^2}(\B (t+1)^2+ \alpha^2\sigma_b^2 O(t^3))=\B+\alpha^2\sigma_b^2 O(t)$. $(ii)$ $\EE{\norm{\eh_t}^2_{A_P}} \approx O(\frac{\B}{\alpha t})+O(\sigma^2_b\alpha)$. $(iii)$ $\EE{\norm{\eh_t}^2_{A_P}}\approx O(\frac{\B}{\alpha t})+O(\frac{1}{t})$.
\end{theorem}
The proof follows by  transforming any $A_P$ into a diagonal matrix, and looking at $1$-dimensional case. \Cref{th:pspd}-$(i)$: the $\frac{1}{(t+1)^2}$ in the denominator of $\EE{\norm{\eh_t}^2}$ is directly a result of iterate averaging,  further, when $t$ is small enough such that the forgetting factor is very close to unity, in which case no forgetting happens, and error accumulates due to the summation of the iterates leading to the $\B(t+1)^2$ and $\alpha^2 \sigma_b^2 O(t^3)$ terms. \Cref{th:pspd}-$(ii)$ is same as the previous except that, now the error is also measured with respect to $A_P$ as result we can achieve a rate of $O(\frac{1}{\sqrt{t}})$ if we choose $\alpha=\frac{1}{\sqrt{t}}$. \Cref{th:pspd}-$(iii)$ is when the noise also scales with $A_P$, and a rate of $O(\frac{1}{t})$ for any fixed $\alpha\in(0,1)$. Note that \Cref{th:pspd}-$iii$ has special structure, i.e., the noise is \emph{scaled} and the mean squared error is measured with respect to the quadratic norm induced by $A_P$, and these result in a uniform finite time rate of $\frac{1}{t}$. We would like to mention that the results of \citet{bach} is very similar to case $(iii)$. Further, it is clear that these special structures are not present in the case of TD algorithms for APE problems.
The next result shows that universal step-sizes need to exist always.
\begin{proposition}\label{prop:unistep}
There does not exist $\alpha$ such that $\Lambda(I-\alpha A_P)<1, \forall P\in \P_{det}$. 
\end{proposition}
In the case of $\P_{det}$,  $\Lambda(I-\alpha A_P)=(1-\alpha u)^2+v^2$ and as $u\ra 0$ the step-size $\alpha\ra 0$ and hence $\alpha_P$ is an empty-set.
\end{comment}

We first consider the question of choosing a \emph{universal} constant step-size which is defined as follows:
\begin{definition} Let $\P$ be a problem class and $P\in\P$ be an instance, then define $\alpha^{stab}_{P}\subset (0,\infty)$ to be such that the random matrix product $\Pi_{s=1}^t (I-\alpha A_s)\ra 0$ (note $(b_t,A_t)\overset{i.i.d}{\sim} P$ ) as $t\ra\infty$ whenever $\alpha\in \alpha^{stab}_P$ and $\alpha^{stab}_{\P}\eqdef\cap_{P\in \P}\alpha_{P}$. Define any $\alpha\in\alpha_{\P}$ to be a \emph{universal} step-size choice for $\P$.
\end{definition}
Note that from \Cref{lem:hur} we have $(0,\alpha_{P_U})\subset \alpha^{stab}_{P}$. 

\textbf{Uniform Rates:} Suppose the set $\alpha^{stab}_{\P}$ is non-empty for a class $\P$, then the question of \emph{uniform} rates boils down to handling the the bias-
variance trade-off in \Cref{th:rate}, say we choose $\alpha\in,\alpha^{stab}_{\P}$: a larger $\alpha$ might mean faster forgetting of bias; a smaller $\alpha$ means lesser noise. In what follows, we investigate the questions of universality and uniformity for simple problem classes define as follows:
\begin{definition}
Define the following \emph{additive noise} problem classes (where $A_t=A_P,\forall t\geq 0$): $(i)$  $\P_{USN}=\{ P: A_P\in (0,1) , b_P=0, \sigma^2_{b_P}\leq 1\}$, $(ii)$ $\P_{SN}\subset \P_{USN}$ such that $\EE{b_t^2}\leq A_P$. $(iii)$ $\P_{rot}=\{P=(u,v): b_t=\mathbf{0}, A_t=A_P, \forall t\geq 0, A_P=\left[\begin{matrix} u &v \\ -v & u\end{matrix}\right], u^2+v^2\leq B\}$.
\end{definition}
Problem classes $\P_{USN}$, $\P_{SN}$ and $\P_{ROT}$ are special cases of \Cref{assmp:lsa}-\eqref{dist}, i.e., they have only \emph{additive} noise through $b_t$. While the additive noise case does not hold for our domains such as RL, we are interested in these classes to understand how problem structure plays a crucial role in universality of step-size and uniformity of rates. In $\P_{USN}$, the noise is not scaled and in $\P_{SN}$ the noise scales with $A_P$.
\begin{theorem}\label{th:pspd}
Any $\alpha\in(0,1)$ is a universal step-size for $\P_{USN}$ and $\P_{SN}$. Let $\B=e_0^2$, then for given $t\geq 1$, we have $(i)$  $\arg\max_{P\in \P_{USN}}\eep{\norm{\eh_t}^2}{P}=\Omega\left( \frac{1}{(t+1)^2}(\B (t+1)^2+ \alpha^2\sigma_b^2 O(t^3))\right)=\Omega\left(\B+\alpha^2\sigma_b^2 t \right)$. $(ii)$ $\arg\max_{P\in \P_{USN}}\eep{\norm{\eh_t}^2_{A_P}}{P} = \Omega\left(\frac{\B}{\alpha t}+\sigma^2_b\alpha\right)$. $(iii)$ $\arg\max_{P\in \P_{SN}}\EE{\norm{\eh_t}^2_{A_P}}= \Omega\left(\frac{\B}{\alpha t}+\frac{1}{t}\right)$.
\end{theorem}
The proof follows by  transforming any $A_P$ into a diagonal matrix, and looking at $1$-dimensional case. \Cref{th:pspd}-$(i)$: the $\frac{1}{(t+1)^2}$ in the denominator of $\EE{\norm{\eh_t}^2}$ is directly a result of iterate averaging,  further, when $t$ is small enough such that the forgetting factor is very close to unity, in which case no forgetting happens, and error accumulates due to the summation of the iterates leading to the $\B(t+1)^2$ and $\alpha^2 \sigma_b^2 O(t^3)$ terms. \Cref{th:pspd}-$(ii)$ is same as the previous except that, now the error is also measured with respect to $A_P$ as result we can achieve a rate of $O(\frac{1}{\sqrt{t}})$ if we choose $\alpha=\frac{1}{\sqrt{t}}$. \Cref{th:pspd}-$(iii)$ is when the noise also scales with $A_P$, and a rate of $O(\frac{1}{t})$ for any fixed $\alpha\in(0,1)$. Note that \Cref{th:pspd}-$iii$ has special structure, i.e., the noise is \emph{scaled} and the mean squared error is measured with respect to the quadratic norm induced by $A_P$, and these result in a uniform finite time rate of $\frac{1}{t}$. We would like to mention that the results of \citet{bach} is very similar to case $(iii)$. Further, it is clear that these special structures are not present in the case of TD algorithms for APE problems.

 In $\P_{rot}$ the matrices are \emph{Hurwitz} and eigenvalues have non-zero imaginary parts. The next result shows that universal step-sizes need to exist always.
\begin{proposition}\label{prop:unistep}
There does not exist $\alpha$ such that $\Lambda(I-\alpha A_P)<1, \forall P\in \P_{ROT}$. 
\end{proposition}
In the case of $\P_{ROT}$,  $\Lambda(I-\alpha A_P)=(1-\alpha u)^2+v^2$ and as $u\ra 0$ the step-size $\alpha\ra 0$ and hence $\alpha_P$ is an empty-set.

\begin{abstract}
Temporal difference learning algorithms such as TD(0) and GTD in reinforcement learning (RL) and the stochastic gradient descent (SGD) for linear prediction are linear stochastic approximation (LSA) algorithms that make only $O(d)$ (parameter dimension) computations per iteration. In the design of LSA algorithms, step-size choice is critical, and is usually tuned for each problem instance. In this paper,  we look at a constant step-size averaged linear stochastic approximation approach (CALSA), and for a class of  problems, we ask whether properties of $i)$ a \emph{universal} constant step-size and $ii)$ a \emph{uniform} fast rate of $\frac{C}{t}$ for the mean square-error hold for all instance of the class, where the constant $C>0$ does not depend on the problem instance. We show that the answer to these question, in general, is \emph{no}. However, we show that for some interesting problem classes in RL, TD algorithms with a problem independent universal constant step-size and iterate averaging, achieve a uniform asymptotic rate of $O(\frac{1}{t})$.  
\end{abstract}
\begin{comment}
We consider $d$-dimensional \emph{linear inversion} problems (arising in machine learning) where the aim is to compute a $\ts\in \R^d$ such that $\ts=A^{-1}b$ using noisy samples of $A\in\R^{\dcd}$ and $b\in\R^d$. Linear stochastic approximation (LSA) is a widely used approach to solve such problems: the stochastic gradient descent procedure to solve linear prediction and the temporal difference class of learning algorithms (such as TD(0) or GTD) for approximate value function estimation in reinforcement learning are LSA algorithms. An important parameter that affects the performance of LSA is the step-size or learning rate. In this paper,  we look at a constant step-size averaged linear stochastic approximation approach (CALSA) to solve a class $\P$ of linear inversion problems, and ask whether a uniformly fast rate of $O(\frac{1}{t})$ is achievable for the mean square-error across all the problem instances. We show that the answer to this question, in general, is \emph{no}. However, we show that instance dependent  finite-time rate of $O(\frac{1}{t})$ is achievable and for some interesting problem classes in reinforcement learning the constant step-size can be chosen in a problem independent manner.
\end{comment}
